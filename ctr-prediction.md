## Предсказание вероятности клика (CTR-prediction)

### Задача
**Дано**   
Исторические данные о показах, кликах с информацией о пользователе, объявлении, месте размещения и т.д.
[(**x**, y)], **x** -- всевозможные признаки пользователя, объявления, места размещения, y in {0, 1} -- был клик или нет

Разработать алгоритм, который для любой пары пользователь, объявление с их признаками рассчитывает вероятность клика пользователя по рекламному объявлению.

### Решения  
* [2020, A Sparse Deep Factorization Machine for Efficient CTR prediction, Yahoo Research](https://arxiv.org/pdf/2002.06987.pdf)  
Модель объединяет компонент FwFM и DNN в единую модель, которая эффективна для обучения взаимодействий признаков как низкого, так и высокого порядка. DeepFwFM позволяет значительно сократить время вывода при использовании такой комбинации, в то время как другие архитектуры могут потерпеть неудачу либо в глубоких ускорениях модели, либо в точных предсказаниях.
Pruning это итеративное удаление связей между нейронами, в простом случае тех, которые имеют наименьший вес, для сокращения размера модели.
Авторы статьи используют structural pruning для ускорения расчета предсказания и оптимизации по памяти через уменьшение количества параметров. А именно они итеративно обрезают связи в DNN компоненте и в матрице R.
В результате качество сравнимое с state-of-the-art, но меньше ресурсов для обучения.

* [2020, Field-weighted Factorization Machine (FwFM), WWW](https://arxiv.org/pdf/1806.03514.pdf)  
Для оптимизации по памяти в FwFM введена матрица R (симметричная матрица размерности n) как взаимодействие различных групп признаков, поэтому более эффективно по памяти,  предсказания также хороши как у FFM, но гораздо меньше параметров для обучения.
    
* [2016, Field-aware Factorization Machine (FFM), RecSys](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf)  
Вводят понятие группы признаков (field). Теперь каждый признак представлен в виде n векторов размерностью k — имеет разное представление в каждой другой группе признаков, что позволяет более точно учитывать взаимодействие между группами признаков. Это значительно повышает эффективность прогнозирования, но количество параметров также значительно увеличивается.

* [2010, Factorization machine (FM), IEEE](https://analyticsconsultores.com.mx/wp-content/uploads/2019/03/Factorization-Machines-Steffen-Rendle-Osaka-University-2010.pdf)   
Учитывают линейные и квадратичные (между любыми двумя признаками) взаимодействия признаков. Для учета квадратичного взаимодействия строят эмбеддинги фич с помощью матричного разложения и используют скалярное произведение эмбеддингов как вес взаимодействия фич.


DNN с эмбеддингами сильнее учитывают нелинейное взаимодействие между признаками. 

* [2018, eXtreme Deep Factorization Machine (XDeepFM), KDD](https://arxiv.org/pdf/1803.05170.pdf)  
XDeepFM предлагает включить сжатую сеть взаимодействий (CIN) для обучения высокоуровнего взаимодействия признаков. Основным недостатком является то, что CIN имеет большую вычислительную сложность, чем DNN, что приводит к дорогостоящим вычислениям в крупных рекламных системах.

* [2017, DeepFM](https://arxiv.org/pdf/1703.04247.pdf)  
DeepFM решил проблему сложной работы с пересечением признаков с помощью моделирования низкоуровнего взаимодействия через FM компонент вместо линейной модели. 

* [2016, Wide & Deep, DLRS](https://dl.acm.org/doi/10.1145/2988450.2988454)  
Предложили обучить объединенную сеть, которая сочетает в себе линейную модель и глубокую модель, чтобы изучать взаимодействия признаков как низкого, так и высокого порядка. Тем не менее, пересечения признаков в линейной модели все еще требуют экспертной работы с ними и не могут быть легко адаптированы к новым наборам данных.
    

    

